{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from spacy.lang.de import German\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join('data','REFINED','ndr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteratefiles(filepath):\n",
    "    list_dir = os.listdir(filepath)\n",
    "    data = []\n",
    "    for item in list_dir:\n",
    "        with open(os.path.join(folder,item),\"r\",encoding='utf-8') as f:\n",
    "            res = f.read() \n",
    "        dict = json.loads(res)\n",
    "        content = dict['content'].values()\n",
    "        c = ''\n",
    "        for l in content: \n",
    "            c += l[1]\n",
    "        content = c\n",
    "        tokens = prepare_text_for_lda(content)\n",
    "        data.append(tokens)\n",
    "    return data\n",
    "\n",
    "text = iteratefiles(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\teres\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "parser = German()\n",
    "\n",
    "def tokenize(text): \n",
    "    # tokenize\n",
    "    tokens = []\n",
    "    t = parser(text)\n",
    "    for token in t:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            tokens.append('URL')\n",
    "        else:\n",
    "            tokens.append(token.lower_)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "def return_lemma(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = set(nltk.corpus.stopwords.words('german'))\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "    tokens = [lemmatize(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iteratefiles(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(data)\n",
    "corpus = [dictionary.doc2bow(token) for token in data]\n",
    "num_topics = 100\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus,num_topics=num_topics,id2word=dictionary,passes=15)\n",
    "ldamodel.save('model5.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38, '0.000*\"virus\" + 0.000*\"schon\" + 0.000*\"sagen\" + 0.000*\"beispiel\" + 0.000*\"immer\" + 0.000*\"natürlich\" + 0.000*\"prozent\" + 0.000*\"vielleicht\" + 0.000*\"einfach\" + 0.000*\"genau\"')\n",
      "(11, '0.000*\"schon\" + 0.000*\"sagen\" + 0.000*\"immer\" + 0.000*\"natürlich\" + 0.000*\"beispiel\" + 0.000*\"virus\" + 0.000*\"einfach\" + 0.000*\"vielleicht\" + 0.000*\"genau\" + 0.000*\"studie\"')\n",
      "(26, '0.000*\"virus\" + 0.000*\"sagen\" + 0.000*\"immer\" + 0.000*\"schon\" + 0.000*\"natürlich\" + 0.000*\"eigentlich\" + 0.000*\"studie\" + 0.000*\"beispiel\" + 0.000*\"einfach\" + 0.000*\"patienten\"')\n",
      "(89, '0.000*\"schon\" + 0.000*\"virus\" + 0.000*\"immer\" + 0.000*\"beispiel\" + 0.000*\"natürlich\" + 0.000*\"sagen\" + 0.000*\"einfach\" + 0.000*\"studie\" + 0.000*\"vielleicht\" + 0.000*\"patienten\"')\n",
      "(34, '0.000*\"schon\" + 0.000*\"sagen\" + 0.000*\"virus\" + 0.000*\"beispiel\" + 0.000*\"einfach\" + 0.000*\"vielleicht\" + 0.000*\"patienten\" + 0.000*\"prozent\" + 0.000*\"wirklich\" + 0.000*\"immer\"')\n",
      "(45, '0.012*\"türklinke\" + 0.010*\"eingeübt\" + 0.008*\"einspielen\" + 0.008*\"hinkommt\" + 0.007*\"metall\" + 0.007*\"station\" + 0.007*\"meter\" + 0.005*\"tropfen\" + 0.005*\"simpel\" + 0.005*\"rechthaberei\"')\n",
      "(74, '0.013*\"schon\" + 0.009*\"einfach\" + 0.009*\"sagen\" + 0.008*\"beispiel\" + 0.008*\"natürlich\" + 0.007*\"immer\" + 0.007*\"prozent\" + 0.007*\"studie\" + 0.006*\"genau\" + 0.006*\"patienten\"')\n",
      "(30, '0.000*\"schon\" + 0.000*\"sagen\" + 0.000*\"vielleicht\" + 0.000*\"einfach\" + 0.000*\"virus\" + 0.000*\"immer\" + 0.000*\"eigentlich\" + 0.000*\"viele\" + 0.000*\"genau\" + 0.000*\"natürlich\"')\n",
      "(24, '0.014*\"sagen\" + 0.013*\"schon\" + 0.010*\"virus\" + 0.009*\"patienten\" + 0.008*\"beispiel\" + 0.008*\"immer\" + 0.008*\"vielleicht\" + 0.007*\"müssen\" + 0.007*\"natürlich\" + 0.006*\"einfach\"')\n",
      "(46, '0.000*\"sagen\" + 0.000*\"schon\" + 0.000*\"virus\" + 0.000*\"eigentlich\" + 0.000*\"beispiel\" + 0.000*\"einfach\" + 0.000*\"prozent\" + 0.000*\"vielleicht\" + 0.000*\"studie\" + 0.000*\"genau\"')\n",
      "(83, '0.000*\"schon\" + 0.000*\"sagen\" + 0.000*\"virus\" + 0.000*\"natürlich\" + 0.000*\"patienten\" + 0.000*\"einfach\" + 0.000*\"vielleicht\" + 0.000*\"genau\" + 0.000*\"beispiel\" + 0.000*\"immer\"')\n",
      "(33, '0.000*\"sagen\" + 0.000*\"schon\" + 0.000*\"immer\" + 0.000*\"virus\" + 0.000*\"patienten\" + 0.000*\"beispiel\" + 0.000*\"natürlich\" + 0.000*\"vielleicht\" + 0.000*\"einfach\" + 0.000*\"eigentlich\"')\n",
      "(1, '0.000*\"virus\" + 0.000*\"schon\" + 0.000*\"sagen\" + 0.000*\"beispiel\" + 0.000*\"immer\" + 0.000*\"einfach\" + 0.000*\"vielleicht\" + 0.000*\"bisschen\" + 0.000*\"viele\" + 0.000*\"genau\"')\n",
      "(31, '0.000*\"virus\" + 0.000*\"schon\" + 0.000*\"sagen\" + 0.000*\"immer\" + 0.000*\"beispiel\" + 0.000*\"einfach\" + 0.000*\"natürlich\" + 0.000*\"patienten\" + 0.000*\"vielleicht\" + 0.000*\"viele\"')\n",
      "(10, '0.024*\"neuropilin\" + 0.012*\"schulalter\" + 0.010*\"lebensjahr\" + 0.010*\"getestete\" + 0.009*\"ferienzeit\" + 0.009*\"child\" + 0.009*\"schulschließen\" + 0.008*\"60.000\" + 0.007*\"teststellen\" + 0.007*\"ruhestandsalter\"')\n",
      "(51, '0.000*\"virus\" + 0.000*\"schon\" + 0.000*\"sagen\" + 0.000*\"immer\" + 0.000*\"vielleicht\" + 0.000*\"einfach\" + 0.000*\"genau\" + 0.000*\"eigentlich\" + 0.000*\"studie\" + 0.000*\"prozent\"')\n",
      "(97, '0.051*\"antikörper\" + 0.015*\"abteilung\" + 0.013*\"schwangerschaftstest\" + 0.012*\"antikörpern\" + 0.011*\"neutralisierenden\" + 0.011*\"entlassen\" + 0.011*\"patienten\" + 0.010*\"nahbereich\" + 0.008*\"nachweisbaren\" + 0.008*\"einatmen\"')\n",
      "(17, '0.000*\"schon\" + 0.000*\"virus\" + 0.000*\"natürlich\" + 0.000*\"sagen\" + 0.000*\"patienten\" + 0.000*\"beispiel\" + 0.000*\"einfach\" + 0.000*\"immer\" + 0.000*\"genau\" + 0.000*\"vielleicht\"')\n",
      "(56, '0.000*\"schon\" + 0.000*\"sagen\" + 0.000*\"virus\" + 0.000*\"immer\" + 0.000*\"eigentlich\" + 0.000*\"beispiel\" + 0.000*\"vielleicht\" + 0.000*\"einfach\" + 0.000*\"prozent\" + 0.000*\"studie\"')\n",
      "(43, '0.000*\"schon\" + 0.000*\"virus\" + 0.000*\"sagen\" + 0.000*\"einfach\" + 0.000*\"vielleicht\" + 0.000*\"studie\" + 0.000*\"müssen\" + 0.000*\"beispiel\" + 0.000*\"natürlich\" + 0.000*\"viele\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69eb92836b941e979072a76c7fcfffe5419cca933cedd02cfafbdfca1a93358c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
