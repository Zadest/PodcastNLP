{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "24d3916f4a9f8bbe764819496dcbc51ff6063af5fca03b7fe0f93e0003d125b6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding : utf8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from textblob_de import TextBlobDE as t\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import _flatten_list\n",
    "\n",
    "from textblob_de.taggers import PatternTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = {}\n",
    "# Listen definieren\n",
    "# Christian Drosten\n",
    "cd = []\n",
    "lencd = []\n",
    "wtypecd = {}\n",
    "countercd = 0\n",
    "# Sandra Ciesek\n",
    "sc = []\n",
    "lensc = []\n",
    "wtypesc = {}\n",
    "countersc = 0\n",
    "# Korinna Hennig\n",
    "kh = []\n",
    "lenkh = []\n",
    "wtypekh = {}\n",
    "counterkh = 0\n",
    "# Anja Martini\n",
    "am = []\n",
    "lenam = []\n",
    "wtypeam = {}\n",
    "counteram = 0\n",
    "# Beke Schulmann\n",
    "bs = []\n",
    "lenbs = []\n",
    "wtypebs = {}\n",
    "counterbs = 0\n",
    "# Iterator\n",
    "people = ['cd','sc','kh','am','bs']\n",
    "mods = ['kh','am','bs']\n",
    "scientist = ['cd','sc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathndr = os.path.join('data','REFINED','ndr')\n",
    "ldndr = os.listdir(pathndr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iele', 'Jüngere', 'erkrankten,', 'die', 'aus', 'dem', 'Urlaub', 'oder', 'von', 'Heimatbesuchen', 'zurückkehren.', 'Das', 'ist', 'sicherlich', 'eine', 'Kombination.', 'Wir', 'müssen', 'das', 'genau', 'beobachten', 'in', 'den', 'nächsten', 'Wochen,', 'ob', 'es', 'auch', 'zu', 'schweren', 'Erkrankungsverläufen', 'bei', 'diesen', 'Neuinfizierten', 'kommt', 'und', 'wie', 'sich', 'die', 'Situation', 'weiterentwickelt.']\n",
      "93\n",
      "792\n",
      "stop\n",
      "['Sars', 'CoV2', 'heißt', 'das', 'neuartige', 'Corona', 'Virus,', 'das', 'uns', 'zurzeit', 'Tag', 'für', 'Tag', 'in', 'den', 'Schlagzeilen', 'beschäftigt.', 'Und', 'die', 'Entwicklung', 'hat', 'sich', 'beschleunigt.', 'Wir', 'haben', 'neue', 'Fälle', 'in', 'Deutschland', 'gemeldet', 'und', 'in', 'anderen', 'europäischen', 'Ländern.', 'Spätestens', 'jetzt', 'ist', 'der', 'Zeitpunkt', 'erreicht,', 'zu', 'dem', 'wir', 'viel,', 'viel', 'Informationen', 'brauchen.', 'Wir', 'sprechen', 'deshalb', 'an', 'dieser', 'Stelle', 'jeden', 'Tag', 'mit', 'dem', 'Forscher,', 'der', 'gemeinsam', 'mit', 'seinem', 'Team', 'das', 'Erbgut', 'des', 'Virus', 'entschlüsselt', 'und', 'veröffentlicht', 'hat.', 'Er', 'hat', 'einen', 'Test', 'zum', 'Nachweis', 'entwickelt', 'und', 'berät', 'auch', 'die', 'Bundesregierung.', 'In', 'unserem', 'täglichen', 'Coronavirus-update:', 'Christian', 'Drosten,', 'Leiter', 'der', 'Virologie', 'an', 'der', 'Berliner', 'Charité.', 'Guten', 'Morgen,', 'Herr', 'Drosten,', 'sind', 'Sie', 'überhaupt', 'zum', 'Schlafen', 'gekommen?', 'Oder', 'haben', 'Sie', 'die', 'halbe', 'Nacht', 'Informationen', 'zusammengetragen?']\n",
      "115\n",
      "44\n",
      "stop\n",
      "['Gibt', 'es', 'einen', 'Fall,', 'der', 'Sie', 'überrascht', 'hat?', '']\n",
      "9\n",
      "739\n",
      "stop\n",
      "['Sie', 'haben', 'gerade', 'schon', 'gesagt,', 'eine', 'Lockerung', 'bei', 'einer', 'Inzidenz', 'unter', '100', 'ist', 'vielleicht', 'ein', 'bisschen', 'früh.', 'Aus', 'Sachsen', 'kommt', 'ein', 'ähnlicher', 'Plan', 'für', 'Lockerungen.', 'Da', 'heißt', 'es,', 'wenn', 'es', 'das', 'Infektionsgeschehen', 'zulässt,', 'dann', 'sollen', 'schon', 'ab', 'dem', '15.', 'Februar', 'die', 'Kindergärten', 'und', 'Grundschulen', 'geöffnet', 'werden', 'und', 'danach', 'auch', 'Bereiche', 'wie', 'zum', 'Beispiel', 'Friseursalons.', 'Da', 'ist', 'der', 'Plan,', 'man', 'wolle', 'dann', 'alle', 'drei', 'bis', 'vier', 'Wochen', 'gucken,', 'beobachten,', 'wie', 'sich', 'das', 'entwickelt', 'und', 'ob', 'das', 'Geschehen', 'noch', 'beherrschbar', 'bleibt.', 'Wenn', 'wir', 'jetzt', 'mal', 'davon', 'ausgehen,', 'wir', 'machen', 'Lockerungen', 'ab', 'einer', 'Inzidenz', 'von', 'unter', '100,', 'würden', 'wir', 'dann', 'damit', 'nicht', 'unseren', 'aktuellen', 'Fortschritt', 'sehr', 'schnell', 'wieder', 'verspielen?', 'Wie', 'sieht', 'es', 'da', 'aus?', 'MEHR', 'BEACHTUNG', 'DES', 'R-WERTES']\n",
      "115\n",
      "136\n",
      "stop\n",
      "['Also', 'ich', 'versuche,', 'wenn‚s', 'geht,', 'nachts', 'zu', 'schlafen.', 'Es', 'gelingt', 'mir', 'nicht', 'immer,', 'aber', 'ich', 'trage', 'dann', 'nachts', 'keine', 'Informationen', 'zusammen.']\n",
      "21\n",
      "609\n",
      "stop\n",
      "['Ja,', 'wir', 'sehen', 'seit', 'ein', 'paar', 'Tagen', 'oder', 'Wochen', 'einen', 'Anstieg', 'der', 'positiven', 'Tests', 'und', 'sehen', 'aber', 'gleichzeitig', 'auch,', 'dass', 'die', 'Lage', 'in', 'den', 'Krankenhäusern', 'noch', 'sehr', 'entspannt', 'ist.', 'Und', 'ich', 'denke,', 'das', 'hat', 'verschiedene', 'Gründe,', 'dass', 'wir', 'einen', 'vermehrten', 'Anstieg', 'haben,', 'zum', 'Teil,', 'weil', 'wir', 'auch', 'mehr', 'testen,', 'aber', 'auch,', 'dass', 'viele', 'Jüngere', 'erkrankten,', 'die', 'aus', 'dem', 'Urlaub', 'oder', 'von', 'Heimatbesuchen', 'zurückkehren.', 'Das', 'ist', 'sicherlich', 'eine', 'Kombination.', 'Wir', 'müssen', 'das', 'genau', 'beobachten', 'in', 'den', 'nächsten', 'Wochen,', 'ob', 'es', 'auch', 'zu', 'schweren', 'Erkrankungsverläufen', 'bei', 'diesen', 'Neuinfizierten', 'kommt', 'und', 'wie', 'sich', 'die', 'Situation', 'weiterentwickelt.']\n",
      "93\n",
      "792\n",
      "stop\n",
      "['Sars', 'CoV2', 'heißt', 'das', 'neuartige', 'Corona', 'Virus,', 'das', 'uns', 'zurzeit', 'Tag', 'für', 'Tag', 'in', 'den', 'Schlagzeilen', 'beschäftigt.', 'Und', 'die', 'Entwicklung', 'hat', 'sich', 'beschleunigt.', 'Wir', 'haben', 'neue', 'Fälle', 'in', 'Deutschland', 'gemeldet', 'und', 'in', 'anderen', 'europäischen', 'Ländern.', 'Spätestens', 'jetzt', 'ist', 'der', 'Zeitpunkt', 'erreicht,', 'zu', 'dem', 'wir', 'viel,', 'viel', 'Informationen', 'brauchen.', 'Wir', 'sprechen', 'deshalb', 'an', 'dieser', 'Stelle', 'jeden', 'Tag', 'mit', 'dem', 'Forscher,', 'der', 'gemeinsam', 'mit', 'seinem', 'Team', 'das', 'Erbgut', 'des', 'Virus', 'entschlüsselt', 'und', 'veröffentlicht', 'hat.', 'Er', 'hat', 'einen', 'Test', 'zum', 'Nachweis', 'entwickelt', 'und', 'berät', 'auch', 'die', 'Bundesregierung.', 'In', 'unserem', 'täglichen', 'Coronavirus-update:', 'Christian', 'Drosten,', 'Leiter', 'der', 'Virologie', 'an', 'der', 'Berliner', 'Charité.', 'Guten', 'Morgen,', 'Herr', 'Drosten,', 'sind', 'Sie', 'überhaupt', 'zum', 'Schlafen', 'gekommen?', 'Oder', 'haben', 'Sie', 'die', 'halbe', 'Nacht', 'Informationen', 'zusammengetragen?']\n",
      "115\n",
      "44\n",
      "stop\n",
      "['Gibt', 'es', 'einen', 'Fall,', 'der', 'Sie', 'überrascht', 'hat?', '']\n",
      "9\n",
      "739\n",
      "stop\n",
      "['Sie', 'haben', 'gerade', 'schon', 'gesagt,', 'eine', 'Lockerung', 'bei', 'einer', 'Inzidenz', 'unter', '100', 'ist', 'vielleicht', 'ein', 'bisschen', 'früh.', 'Aus', 'Sachsen', 'kommt', 'ein', 'ähnlicher', 'Plan', 'für', 'Lockerungen.', 'Da', 'heißt', 'es,', 'wenn', 'es', 'das', 'Infektionsgeschehen', 'zulässt,', 'dann', 'sollen', 'schon', 'ab', 'dem', '15.', 'Februar', 'die', 'Kindergärten', 'und', 'Grundschulen', 'geöffnet', 'werden', 'und', 'danach', 'auch', 'Bereiche', 'wie', 'zum', 'Beispiel', 'Friseursalons.', 'Da', 'ist', 'der', 'Plan,', 'man', 'wolle', 'dann', 'alle', 'drei', 'bis', 'vier', 'Wochen', 'gucken,', 'beobachten,', 'wie', 'sich', 'das', 'entwickelt', 'und', 'ob', 'das', 'Geschehen', 'noch', 'beherrschbar', 'bleibt.', 'Wenn', 'wir', 'jetzt', 'mal', 'davon', 'ausgehen,', 'wir', 'machen', 'Lockerungen', 'ab', 'einer', 'Inzidenz', 'von', 'unter', '100,', 'würden', 'wir', 'dann', 'damit', 'nicht', 'unseren', 'aktuellen', 'Fortschritt', 'sehr', 'schnell', 'wieder', 'verspielen?', 'Wie', 'sieht', 'es', 'da', 'aus?', 'MEHR', 'BEACHTUNG', 'DES', 'R-WERTES']\n",
      "115\n",
      "136\n",
      "stop\n",
      "['Also', 'ich', 'versuche,', 'wenn‚s', 'geht,', 'nachts', 'zu', 'schlafen.', 'Es', 'gelingt', 'mir', 'nicht', 'immer,', 'aber', 'ich', 'trage', 'dann', 'nachts', 'keine', 'Informationen', 'zusammen.']\n",
      "21\n",
      "609\n",
      "stop\n",
      "['Ja,', 'wir', 'sehen', 'seit', 'ein', 'paar', 'Tagen', 'oder', 'Wochen', 'einen', 'Anstieg', 'der', 'positiven', 'Tests', 'und', 'sehen', 'aber', 'gleichzeitig', 'auch,', 'dass', 'die', 'Lage', 'in', 'den', 'Krankenhäusern', 'noch', 'sehr', 'entspannt', 'ist.', 'Und', 'ich', 'denke,', 'das', 'hat', 'verschiedene', 'Gründe,', 'dass', 'wir', 'einen', 'vermehrten', 'Anstieg', 'haben,', 'zum', 'Teil,', 'weil', 'wir', 'auch', 'mehr', 'testen,', 'aber', 'auch,', 'dass', 'viele', 'Jüngere', 'erkrankten,', 'die', 'aus', 'dem', 'Urlaub', 'oder', 'von', 'Heimatbesuchen', 'zurückkehren.', 'Das', 'ist', 'sicherlich', 'eine', 'Kombination.', 'Wir', 'müssen', 'das', 'genau', 'beobachten', 'in', 'den', 'nächsten', 'Wochen,', 'ob', 'es', 'auch', 'zu', 'schweren', 'Erkrankungsverläufen', 'bei', 'diesen', 'Neuinfizierten', 'kommt', 'und', 'wie', 'sich', 'die', 'Situation', 'weiterentwickelt.']\n",
      "93\n",
      "792\n",
      "stop\n",
      "['Sars', 'CoV2', 'heißt', 'das', 'neuartige', 'Corona', 'Virus,', 'das', 'uns', 'zurzeit', 'Tag', 'für', 'Tag', 'in', 'den', 'Schlagzeilen', 'beschäftigt.', 'Und', 'die', 'Entwicklung', 'hat', 'sich', 'beschleunigt.', 'Wir', 'haben', 'neue', 'Fälle', 'in', 'Deutschland', 'gemeldet', 'und', 'in', 'anderen', 'europäischen', 'Ländern.', 'Spätestens', 'jetzt', 'ist', 'der', 'Zeitpunkt', 'erreicht,', 'zu', 'dem', 'wir', 'viel,', 'viel', 'Informationen', 'brauchen.', 'Wir', 'sprechen', 'deshalb', 'an', 'dieser', 'Stelle', 'jeden', 'Tag', 'mit', 'dem', 'Forscher,', 'der', 'gemeinsam', 'mit', 'seinem', 'Team', 'das', 'Erbgut', 'des', 'Virus', 'entschlüsselt', 'und', 'veröffentlicht', 'hat.', 'Er', 'hat', 'einen', 'Test', 'zum', 'Nachweis', 'entwickelt', 'und', 'berät', 'auch', 'die', 'Bundesregierung.', 'In', 'unserem', 'täglichen', 'Coronavirus-update:', 'Christian', 'Drosten,', 'Leiter', 'der', 'Virologie', 'an', 'der', 'Berliner', 'Charité.', 'Guten', 'Morgen,', 'Herr', 'Drosten,', 'sind', 'Sie', 'überhaupt', 'zum', 'Schlafen', 'gekommen?', 'Oder', 'haben', 'Sie', 'die', 'halbe', 'Nacht', 'Informationen', 'zusammengetragen?']\n",
      "115\n",
      "44\n",
      "stop\n",
      "['Gibt', 'es', 'einen', 'Fall,', 'der', 'Sie', 'überrascht', 'hat?', '']\n",
      "9\n",
      "739\n",
      "stop\n",
      "['Sie', 'haben', 'gerade', 'schon', 'gesagt,', 'eine', 'Lockerung', 'bei', 'einer', 'Inzidenz', 'unter', '100', 'ist', 'vielleicht', 'ein', 'bisschen', 'früh.', 'Aus', 'Sachsen', 'kommt', 'ein', 'ähnlicher', 'Plan', 'für', 'Lockerungen.', 'Da', 'heißt', 'es,', 'wenn', 'es', 'das', 'Infektionsgeschehen', 'zulässt,', 'dann', 'sollen', 'schon', 'ab', 'dem', '15.', 'Februar', 'die', 'Kindergärten', 'und', 'Grundschulen', 'geöffnet', 'werden', 'und', 'danach', 'auch', 'Bereiche', 'wie', 'zum', 'Beispiel', 'Friseursalons.', 'Da', 'ist', 'der', 'Plan,', 'man', 'wolle', 'dann', 'alle', 'drei', 'bis', 'vier', 'Wochen', 'gucken,', 'beobachten,', 'wie', 'sich', 'das', 'entwickelt', 'und', 'ob', 'das', 'Geschehen', 'noch', 'beherrschbar', 'bleibt.', 'Wenn', 'wir', 'jetzt', 'mal', 'davon', 'ausgehen,', 'wir', 'machen', 'Lockerungen', 'ab', 'einer', 'Inzidenz', 'von', 'unter', '100,', 'würden', 'wir', 'dann', 'damit', 'nicht', 'unseren', 'aktuellen', 'Fortschritt', 'sehr', 'schnell', 'wieder', 'verspielen?', 'Wie', 'sieht', 'es', 'da', 'aus?', 'MEHR', 'BEACHTUNG', 'DES', 'R-WERTES']\n",
      "115\n",
      "136\n",
      "stop\n",
      "['Also', 'ich', 'versuche,', 'wenn‚s', 'geht,', 'nachts', 'zu', 'schlafen.', 'Es', 'gelingt', 'mir', 'nicht', 'immer,', 'aber', 'ich', 'trage', 'dann', 'nachts', 'keine', 'Informationen', 'zusammen.']\n",
      "21\n",
      "609\n",
      "stop\n",
      "['Ja,', 'wir', 'sehen', 'seit', 'ein', 'paar', 'Tagen', 'oder', 'Wochen', 'einen', 'Anstieg', 'der', 'positiven', 'Tests', 'und', 'sehen', 'aber', 'gleichzeitig', 'auch,', 'dass', 'die', 'Lage', 'in', 'den', 'Krankenhäusern', 'noch', 'sehr', 'entspannt', 'ist.', 'Und', 'ich', 'denke,', 'das', 'hat', 'verschiedene', 'Gründe,', 'dass', 'wir', 'einen', 'vermehrten', 'Anstieg', 'haben,', 'zum', 'Teil,', 'weil', 'wir', 'auch', 'mehr', 'testen,', 'aber', 'auch,', 'dass', 'viele', 'Jüngere', 'erkrankten,', 'die', 'aus', 'dem', 'Urlaub', 'oder', 'von', 'Heimatbesuchen', 'zurückkehren.', 'Das', 'ist', 'sicherlich', 'eine', 'Kombination.', 'Wir', 'müssen', 'das', 'genau', 'beobachten', 'in', 'den', 'nächsten', 'Wochen,', 'ob', 'es', 'auch', 'zu', 'schweren', 'Erkrankungsverläufen', 'bei', 'diesen', 'Neuinfizierten', 'kommt', 'und', 'wie', 'sich', 'die', 'Situation', 'weiterentwickelt.']\n",
      "93\n",
      "792\n",
      "stop\n",
      "['Sars', 'CoV2', 'heißt', 'das', 'neuartige', 'Corona', 'Virus,', 'das', 'uns', 'zurzeit', 'Tag', 'für', 'Tag', 'in', 'den', 'Schlagzeilen', 'beschäftigt.', 'Und', 'die', 'Entwicklung', 'hat', 'sich', 'beschleunigt.', 'Wir', 'haben', 'neue', 'Fälle', 'in', 'Deutschland', 'gemeldet', 'und', 'in', 'anderen', 'europäischen', 'Ländern.', 'Spätestens', 'jetzt', 'ist', 'der', 'Zeitpunkt', 'erreicht,', 'zu', 'dem', 'wir', 'viel,', 'viel', 'Informationen', 'brauchen.', 'Wir', 'sprechen', 'deshalb', 'an', 'dieser', 'Stelle', 'jeden', 'Tag', 'mit', 'dem', 'Forscher,', 'der', 'gemeinsam', 'mit', 'seinem', 'Team', 'das', 'Erbgut', 'des', 'Virus', 'entschlüsselt', 'und', 'veröffentlicht', 'hat.', 'Er', 'hat', 'einen', 'Test', 'zum', 'Nachweis', 'entwickelt', 'und', 'berät', 'auch', 'die', 'Bundesregierung.', 'In', 'unserem', 'täglichen', 'Coronavirus-update:', 'Christian', 'Drosten,', 'Leiter', 'der', 'Virologie', 'an', 'der', 'Berliner', 'Charité.', 'Guten', 'Morgen,', 'Herr', 'Drosten,', 'sind', 'Sie', 'überhaupt', 'zum', 'Schlafen', 'gekommen?', 'Oder', 'haben', 'Sie', 'die', 'halbe', 'Nacht', 'Informationen', 'zusammengetragen?']\n",
      "115\n",
      "44\n",
      "stop\n",
      "['Gibt', 'es', 'einen', 'Fall,', 'der', 'Sie', 'überrascht', 'hat?', '']\n",
      "9\n",
      "739\n",
      "stop\n",
      "['Sie', 'haben', 'gerade', 'schon', 'gesagt,', 'eine', 'Lockerung', 'bei', 'einer', 'Inzidenz', 'unter', '100', 'ist', 'vielleicht', 'ein', 'bisschen', 'früh.', 'Aus', 'Sachsen', 'kommt', 'ein', 'ähnlicher', 'Plan', 'für', 'Lockerungen.', 'Da', 'heißt', 'es,', 'wenn', 'es', 'das', 'Infektionsgeschehen', 'zulässt,', 'dann', 'sollen', 'schon', 'ab', 'dem', '15.', 'Februar', 'die', 'Kindergärten', 'und', 'Grundschulen', 'geöffnet', 'werden', 'und', 'danach', 'auch', 'Bereiche', 'wie', 'zum', 'Beispiel', 'Friseursalons.', 'Da', 'ist', 'der', 'Plan,', 'man', 'wolle', 'dann', 'alle', 'drei', 'bis', 'vier', 'Wochen', 'gucken,', 'beobachten,', 'wie', 'sich', 'das', 'entwickelt', 'und', 'ob', 'das', 'Geschehen', 'noch', 'beherrschbar', 'bleibt.', 'Wenn', 'wir', 'jetzt', 'mal', 'davon', 'ausgehen,', 'wir', 'machen', 'Lockerungen', 'ab', 'einer', 'Inzidenz', 'von', 'unter', '100,', 'würden', 'wir', 'dann', 'damit', 'nicht', 'unseren', 'aktuellen', 'Fortschritt', 'sehr', 'schnell', 'wieder', 'verspielen?', 'Wie', 'sieht', 'es', 'da', 'aus?', 'MEHR', 'BEACHTUNG', 'DES', 'R-WERTES']\n",
      "115\n",
      "136\n",
      "stop\n",
      "['Also', 'ich', 'versuche,', 'wenn‚s', 'geht,', 'nachts', 'zu', 'schlafen.', 'Es', 'gelingt', 'mir', 'nicht', 'immer,', 'aber', 'ich', 'trage', 'dann', 'nachts', 'keine', 'Informationen', 'zusammen.']\n",
      "21\n",
      "609\n",
      "stop\n",
      "['Ja,', 'wir', 'sehen', 'seit', 'ein', 'paar', 'Tagen', 'oder', 'Wochen', 'einen', 'Anstieg', 'der', 'positiven', 'Tests', 'und', 'sehen', 'aber', 'gleichzeitig', 'auch,', 'dass', 'die', 'Lage', 'in', 'den', 'Krankenhäusern', 'noch', 'sehr', 'entspannt', 'ist.', 'Und', 'ich', 'denke,', 'das', 'hat', 'verschiedene', 'Gründe,', 'dass', 'wir', 'einen', 'vermehrten', 'Anstieg', 'haben,', 'zum', 'Teil,', 'weil', 'wir', 'auch', 'mehr', 'testen,', 'aber', 'auch,', 'dass', 'viele', 'Jüngere', 'erkrankten,', 'die', 'aus', 'dem', 'Urlaub', 'oder', 'von', 'Heimatbesuchen', 'zurückkehren.', 'Das', 'ist', 'sicherlich', 'eine', 'Kombination.', 'Wir', 'müssen', 'das', 'genau', 'beobachten', 'in', 'den', 'nächsten', 'Wochen,', 'ob', 'es', 'auch', 'zu', 'schweren', 'Erkrankungsverläufen', 'bei', 'diesen', 'Neuinfizierten', 'kommt', 'und', 'wie', 'sich', 'die', 'Situation', 'weiterentwickelt.']\n",
      "93\n",
      "792\n",
      "stop\n",
      "['Sars', 'CoV2', 'heißt', 'das', 'neuartige', 'Corona', 'Virus,', 'das', 'uns', 'zurzeit', 'Tag', 'für', 'Tag', 'in', 'den', 'Schlagzeilen', 'beschäftigt.', 'Und', 'die', 'Entwicklung', 'hat', 'sich', 'beschleunigt.', 'Wir', 'haben', 'neue', 'Fälle', 'in', 'Deutschland', 'gemeldet', 'und', 'in', 'anderen', 'europäischen', 'Ländern.', 'Spätestens', 'jetzt', 'ist', 'der', 'Zeitpunkt', 'erreicht,', 'zu', 'dem', 'wir', 'viel,', 'viel', 'Informationen', 'brauchen.', 'Wir', 'sprechen', 'deshalb', 'an', 'dieser', 'Stelle', 'jeden', 'Tag', 'mit', 'dem', 'Forscher,', 'der', 'gemeinsam', 'mit', 'seinem', 'Team', 'das', 'Erbgut', 'des', 'Virus', 'entschlüsselt', 'und', 'veröffentlicht', 'hat.', 'Er', 'hat', 'einen', 'Test', 'zum', 'Nachweis', 'entwickelt', 'und', 'berät', 'auch', 'die', 'Bundesregierung.', 'In', 'unserem', 'täglichen', 'Coronavirus-update:', 'Christian', 'Drosten,', 'Leiter', 'der', 'Virologie', 'an', 'der', 'Berliner', 'Charité.', 'Guten', 'Morgen,', 'Herr', 'Drosten,', 'sind', 'Sie', 'überhaupt', 'zum', 'Schlafen', 'gekommen?', 'Oder', 'haben', 'Sie', 'die', 'halbe', 'Nacht', 'Informationen', 'zusammengetragen?']\n",
      "115\n",
      "44\n",
      "stop\n",
      "['Gibt', 'es', 'einen', 'Fall,', 'der', 'Sie', 'überrascht', 'hat?', '']\n",
      "9\n",
      "739\n",
      "stop\n",
      "['Sie', 'haben', 'gerade', 'schon', 'gesagt,', 'eine', 'Lockerung', 'bei', 'einer', 'Inzidenz', 'unter', '100', 'ist', 'vielleicht', 'ein', 'bisschen', 'früh.', 'Aus', 'Sachsen', 'kommt', 'ein', 'ähnlicher', 'Plan', 'für', 'Lockerungen.', 'Da', 'heißt', 'es,', 'wenn', 'es', 'das', 'Infektionsgeschehen', 'zulässt,', 'dann', 'sollen', 'schon', 'ab', 'dem', '15.', 'Februar', 'die', 'Kindergärten', 'und', 'Grundschulen', 'geöffnet', 'werden', 'und', 'danach', 'auch', 'Bereiche', 'wie', 'zum', 'Beispiel', 'Friseursalons.', 'Da', 'ist', 'der', 'Plan,', 'man', 'wolle', 'dann', 'alle', 'drei', 'bis', 'vier', 'Wochen', 'gucken,', 'beobachten,', 'wie', 'sich', 'das', 'entwickelt', 'und', 'ob', 'das', 'Geschehen', 'noch', 'beherrschbar', 'bleibt.', 'Wenn', 'wir', 'jetzt', 'mal', 'davon', 'ausgehen,', 'wir', 'machen', 'Lockerungen', 'ab', 'einer', 'Inzidenz', 'von', 'unter', '100,', 'würden', 'wir', 'dann', 'damit', 'nicht', 'unseren', 'aktuellen', 'Fortschritt', 'sehr', 'schnell', 'wieder', 'verspielen?', 'Wie', 'sieht', 'es', 'da', 'aus?', 'MEHR', 'BEACHTUNG', 'DES', 'R-WERTES']\n",
      "115\n",
      "136\n",
      "stop\n",
      "['Also', 'ich', 'versuche,', 'wenn‚s', 'geht,', 'nachts', 'zu', 'schlafen.', 'Es', 'gelingt', 'mir', 'nicht', 'immer,', 'aber', 'ich', 'trage', 'dann', 'nachts', 'keine', 'Informationen', 'zusammen.']\n",
      "21\n",
      "609\n",
      "stop\n",
      "['Ja,', 'wir', 'sehen', 'seit', 'ein', 'paar', 'Tagen', 'oder', 'Wochen', 'einen', 'Anstieg', 'der', 'positiven', 'Tests', 'und', 'sehen', 'aber', 'gleichzeitig', 'auch,', 'dass', 'die', 'Lage', 'in', 'den', 'Krankenhäusern', 'noch', 'sehr', 'entspannt', 'ist.', 'Und', 'ich', 'denke,', 'das', 'hat', 'verschiedene', 'Gründe,', 'dass', 'wir', 'einen', 'vermehrten', 'Anstieg', 'haben,', 'zum', 'Teil,', 'weil', 'wir', 'auch', 'mehr', 'testen,', 'aber', 'auch,', 'dass', 'viele', 'Jüngere', 'erkrankten,', 'die', 'aus', 'dem', 'Urlaub', 'oder', 'von', 'Heimatbesuchen', 'zurückkehren.', 'Das', 'ist', 'sicherlich', 'eine', 'Kombination.', 'Wir', 'müssen', 'das', 'genau', 'beobachten', 'in', 'den', 'nächsten', 'Wochen,', 'ob', 'es', 'auch', 'zu', 'schweren', 'Erkrankungsverläufen', 'bei', 'diesen', 'Neuinfizierten', 'kommt', 'und', 'wie', 'sich', 'die', 'Situation', 'weiterentwickelt.']\n",
      "93\n",
      "792\n",
      "stop\n",
      "['Sars', 'CoV2', 'heißt', 'das', 'neuartige', 'Corona', 'Virus,', 'das', 'uns', 'zurzeit', 'Tag', 'für', 'Tag', 'in', 'den', 'Schlagzeilen', 'beschäftigt.', 'Und', 'die', 'Entwicklung', 'hat', 'sich', 'beschleunigt.', 'Wir', 'haben', 'neue', 'Fälle', 'in', 'Deutschland', 'gemeldet', 'und', 'in', 'anderen', 'europäischen', 'Ländern.', 'Spätestens', 'jetzt', 'ist', 'der', 'Zeitpunkt', 'erreicht,', 'zu', 'dem', 'wir', 'viel,', 'viel', 'Informationen', 'brauchen.', 'Wir', 'sprechen', 'deshalb', 'an', 'dieser', 'Stelle', 'jeden', 'Tag', 'mit', 'dem', 'Forscher,', 'der', 'gemeinsam', 'mit', 'seinem', 'Team', 'das', 'Erbgut', 'des', 'Virus', 'entschlüsselt', 'und', 'veröffentlicht', 'hat.', 'Er', 'hat', 'einen', 'Test', 'zum', 'Nachweis', 'entwickelt', 'und', 'berät', 'auch', 'die', 'Bundesregierung.', 'In', 'unserem', 'täglichen', 'Coronavirus-update:', 'Christian', 'Drosten,', 'Leiter', 'der', 'Virologie', 'an', 'der', 'Berliner', 'Charité.', 'Guten', 'Morgen,', 'Herr', 'Drosten,', 'sind', 'Sie', 'überhaupt', 'zum', 'Schlafen', 'gekommen?', 'Oder', 'haben', 'Sie', 'die', 'halbe', 'Nacht', 'Informationen', 'zusammengetragen?']\n",
      "115\n",
      "44\n",
      "stop\n",
      "['Gibt', 'es', 'einen', 'Fall,', 'der', 'Sie', 'überrascht', 'hat?', '']\n",
      "9\n",
      "739\n",
      "stop\n",
      "['Sie', 'haben', 'gerade', 'schon', 'gesagt,', 'eine', 'Lockerung', 'bei', 'einer', 'Inzidenz', 'unter', '100', 'ist', 'vielleicht', 'ein', 'bisschen', 'früh.', 'Aus', 'Sachsen', 'kommt', 'ein', 'ähnlicher', 'Plan', 'für', 'Lockerungen.', 'Da', 'heißt', 'es,', 'wenn', 'es', 'das', 'Infektionsgeschehen', 'zulässt,', 'dann', 'sollen', 'schon', 'ab', 'dem', '15.', 'Februar', 'die', 'Kindergärten', 'und', 'Grundschulen', 'geöffnet', 'werden', 'und', 'danach', 'auch', 'Bereiche', 'wie', 'zum', 'Beispiel', 'Friseursalons.', 'Da', 'ist', 'der', 'Plan,', 'man', 'wolle', 'dann', 'alle', 'drei', 'bis', 'vier', 'Wochen', 'gucken,', 'beobachten,', 'wie', 'sich', 'das', 'entwickelt', 'und', 'ob', 'das', 'Geschehen', 'noch', 'beherrschbar', 'bleibt.', 'Wenn', 'wir', 'jetzt', 'mal', 'davon', 'ausgehen,', 'wir', 'machen', 'Lockerungen', 'ab', 'einer', 'Inzidenz', 'von', 'unter', '100,', 'würden', 'wir', 'dann', 'damit', 'nicht', 'unseren', 'aktuellen', 'Fortschritt', 'sehr', 'schnell', 'wieder', 'verspielen?', 'Wie', 'sieht', 'es', 'da', 'aus?', 'MEHR', 'BEACHTUNG', 'DES', 'R-WERTES']\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(ldndr)):\n",
    "    path = os.path.join(pathndr,ldndr[i])\n",
    "    if i!=174 and i!=154 and re.search('.json',ldndr[i]):\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            dict1 = json.load(f)\n",
    "            for i in dict1['metadata']['speakers']:\n",
    "                if 'Christian Drosten' in dict1['metadata']['speakers']:\n",
    "                    countercd += 1\n",
    "                if 'Sandra Ciesek' in dict1['metadata']['speakers']:\n",
    "                    countersc += 1\n",
    "                if 'Korinna Hennig' in dict1['metadata']['speakers']:\n",
    "                    counterkh += 1\n",
    "                if 'Anja Martini' in dict1['metadata']['speakers']:\n",
    "                    counteram += 1\n",
    "                if 'Beke Schulmann' in dict1['metadata']['speakers']:\n",
    "                    counterbs += 1  \n",
    "        for i in range(0,len(people)):\n",
    "            for key in dict1:\n",
    "                # if dict1['content'][key][0]=='Christian Drosten':\n",
    "                #     cd.append(dict1['content'][key][1])\n",
    "                # if dict1['content'][key][0]=='Sandra Ciesek':\n",
    "                #     sc.append(dict1['content'][key][1])\n",
    "                # if dict1['content'][key][0]=='Korinna Hennig':\n",
    "                #     kh.append(dict1['content'][key][1])\n",
    "                # if dict1['content'][key][0]=='Anja Martini':\n",
    "                #     am.append(dict1['content'][key][1])\n",
    "                # if dict1['content'][key][0]=='Beke Schulmann':\n",
    "                #     bs.append(dict1[key][1])\n",
    "                \n",
    "                if dict1[key][0]=='Christian Drosten':\n",
    "                    cd.append(dict1[key][1])\n",
    "                if dict1[key][0]=='Sandra Ciesek':\n",
    "                    sc.append(dict1[key][1])\n",
    "                if dict1[key][0]=='Korinna Hennig':\n",
    "                    kh.append(dict1[key][1])\n",
    "                if dict1[key][0]=='Anja Martini':\n",
    "                    am.append(dict1[key][1])\n",
    "                if dict1[key][0]=='Beke Schulmann':\n",
    "                    bs.append(dict1[key][1])\n",
    "                    \n",
    "for i in range(0,len(scientist)):\n",
    "    l = []\n",
    "    if scientist[i]=='cd' and len(cd)>1 :\n",
    "        l.append(cd[i].split(' '))\n",
    "        l = _flatten_list(l)\n",
    "        lencd.append(len(l))\n",
    "    if scientist[i]=='sc'and len(sc)>1:\n",
    "        l.append(sc[i].split(' '))\n",
    "        l = _flatten_list(l)\n",
    "        lensc.append(len(l))\n",
    "\n",
    "for i in range(0,len(mods)):\n",
    "    l = []\n",
    "    if mods[i]=='kh'and len(kh)>1:\n",
    "        l.append(kh[i].split(' '))\n",
    "        l = _flatten_list(l)\n",
    "        lenkh.append(len(l))\n",
    "    if mods[i]=='am'and len(am)>1:\n",
    "        l.append(am[i].split(' '))\n",
    "        l = _flatten_list(l)\n",
    "        lenam.append(len(l))\n",
    "    if mods[i]=='bs'and len(bs)>1:\n",
    "        l.append(bs[i].split(' '))\n",
    "        l = _flatten_list(l)\n",
    "        lenbs.append(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Durchschnittliche Redelänge\n",
    "# Christian Drosten\n",
    "if countercd>1:\n",
    "    avglencd = lencd/countercd\n",
    "# Sandra Ciesek\n",
    "if countersc>1:\n",
    "    avglensc = lensc/countersc\n",
    "# Korinna Hennig\n",
    "if counterkh>1:\n",
    "    avglenkh = lenkh/counterkh\n",
    "# Anja Martini\n",
    "if counteram>1:\n",
    "    avglenam = lenam/counteram\n",
    "# Beke Schulmann\n",
    "if counterbs>1:\n",
    "    avglenbs = lenbs/counterbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gesamten Text für alle Personen zusammenjoinen\n",
    "tcd = \"\\n\".join(cd)\n",
    "tsc = \"\\n\".join(sc)\n",
    "tkh = \"\\n\".join(kh)\n",
    "tam = \"\\n\".join(am)\n",
    "tbs = \"\\n\".join(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a9c6a3710634>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmods\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'am'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos_tagger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tags\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwtypeam\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\textblob\\decorators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\textblob_de\\blob.py\u001b[0m in \u001b[0;36mpos_tags\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m    475\u001b[0m         return [(Word(word, pos_tag=t), unicode(t))\n\u001b[1;32m--> 476\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m                 \u001b[1;31m# new keyword PatternTagger(include_punc=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m                 \u001b[1;31m# if not PUNCTUATION_REGEX.match(unicode(t))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\textblob_de\\taggers.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, sentence, tokenize)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Sentence is tokenized before it is passed on to pattern.de.tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m# (i.e. it is either submitted tokenized or if )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         _tagged = pattern_tag(sentence, tokenize=False,\n\u001b[0m\u001b[0;32m     88\u001b[0m                               \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                               tagset=self.tagset)\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\textblob_de\\ext\\_pattern\\text\\de\\__init__.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(s, tokenize, encoding, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m     \"\"\"\n\u001b[0;32m    258\u001b[0m     \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\textblob_de\\ext\\_pattern\\text\\de\\__init__.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(s, *args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \"\"\" Returns a tagged Unicode string.\n\u001b[0;32m    242\u001b[0m     \"\"\"\n\u001b[1;32m--> 243\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparsetree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\textblob_de\\ext\\_pattern\\text\\__init__.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, s, tokenize, tags, chunks, relations, lemmata, encoding, **kwargs)\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[1;31m# Tagger (required by chunker, labeler & lemmatizer).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtags\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mchunks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mrelations\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlemmata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                 \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\textblob_de\\ext\\_pattern\\text\\de\\__init__.py\u001b[0m in \u001b[0;36mfind_tags\u001b[1;34m(self, tokens, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;31m# We restore the \"ß\" after parsing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mtokens_ss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu\"ß\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ss\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtokens_ss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens_ss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtokens_ss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\textblob_de\\ext\\_pattern\\text\\__init__.py\u001b[0m in \u001b[0;36mfind_tags\u001b[1;34m(self, tokens, **kwargs)\u001b[0m\n\u001b[0;32m    733\u001b[0m         \"\"\"\n\u001b[0;32m    734\u001b[0m         \u001b[1;31m# [\"The\", \"cat\", \"purs\"] => [[\"The\", \"DT\"], [\"cat\", \"NN\"], [\"purs\", \"VB\"]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 735\u001b[1;33m         return find_tags(tokens,\n\u001b[0m\u001b[0;32m    736\u001b[0m                     \u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m   \u001b[1;34m\"lexicon\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m                       \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m     \u001b[1;34m\"model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\textblob_de\\ext\\_pattern\\text\\__init__.py\u001b[0m in \u001b[0;36mfind_tags\u001b[1;34m(tokens, lexicon, model, morphology, context, entities, default, language, map, **kwargs)\u001b[0m\n\u001b[0;32m   1105\u001b[0m             \u001b[1;31m# Use suffix rules (e.g., -ly = RB).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmorphology\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m                 \u001b[0mtagged\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmorphology\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1108\u001b[0m             \u001b[1;31m# Use suffix rules (English default).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"en\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\textblob_de\\ext\\_pattern\\text\\__init__.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, token, previous, next)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"word\"\u001b[0m       \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"char\"\u001b[0m       \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"haspref\"\u001b[0m    \u001b[1;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"hassuf\"\u001b[0m     \u001b[1;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"addpref\"\u001b[0m    \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mknown\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instanz des Taggers erzeugen \n",
    "tagger = PatternTagger()\n",
    "# Nomen, Adjektive/Adverb, Verben https://pythonexamples.org/nltk-pos-tagging/\n",
    "# Nomen und Verben / Satz\n",
    "\n",
    "# Summe Worte\n",
    "wcd = []\n",
    "wsc = []\n",
    "wkh = []\n",
    "wam = []\n",
    "wbs = []\n",
    "# Worttypen\n",
    "wtcd = []\n",
    "wtsc = []\n",
    "wtkh = []\n",
    "wtam = []\n",
    "wtbs = []\n",
    "# Anteil Worttyp/Summe Worte\n",
    "partswcd = {}\n",
    "partswsc = {}\n",
    "partswkh = {}\n",
    "partswam = {}\n",
    "partswbs = {}\n",
    "# Worttypen Anzahl gesamt \n",
    "for i in range(0,len(scientist)):\n",
    "    if scientist[i]=='cd':\n",
    "        blob = t(tcd,pos_tagger=tagger)\n",
    "        for tag in blob.pos_tags:\n",
    "            key = tag[1]\n",
    "            if key in wtypecd:\n",
    "                wtypecd[key] += 1\n",
    "            else:\n",
    "                wtypecd[key] = 1\n",
    "        for key in wtypecd:\n",
    "            wcd.append(wtypecd[key])\n",
    "            wtcd.append(key)\n",
    "        sumwcd = sum(wcd) # Summe aller Wörter\n",
    "    if scientist[i]=='sc':\n",
    "        blob = t(tsc,pos_tagger=tagger)\n",
    "        for tag in blob.pos_tags:\n",
    "            key = tag[1]\n",
    "            if key in wtypesc:\n",
    "                wtypesc[key] += 1\n",
    "            else:\n",
    "                wtypesc[key] = 1\n",
    "        for key in wtypesc:\n",
    "            wsc.append(wtypesc[key])\n",
    "            wtsc.append(key)\n",
    "        sumwsc = sum(wsc) # Summe aller Wörter\n",
    "        \n",
    "for i in range(0,len(mods)):\n",
    "    if mods[i]=='kh':\n",
    "        blob = t(tkh,pos_tagger=tagger)\n",
    "        for tag in blob.pos_tags:\n",
    "            key = tag[1]\n",
    "            if key in wtypekh:\n",
    "                wtypekh[key] += 1\n",
    "            else:\n",
    "                wtypekh[key] = 1\n",
    "        for key in wtypekh:\n",
    "            wkh.append(wtypekh[key])\n",
    "            wtkh.append(key)\n",
    "        sumwak = sum(wak) # Summe aller Wörter\n",
    "    if mods[i]=='am':\n",
    "        blob = t(tam,pos_tagger=tagger)\n",
    "        for tag in blob.pos_tags:\n",
    "            key = tag[1]\n",
    "            if key in wtypeam:\n",
    "                wtypeam[key] += 1\n",
    "            else:\n",
    "                wtypeam[key] = 1\n",
    "        for key in wtypeam:\n",
    "            wam.append(wtypeam[key])\n",
    "            wtam.append(key)\n",
    "        sumwak = sum(wak) # Summe aller Wörter\n",
    "    if mods[i]=='bs':\n",
    "        blob = t(tbs,pos_tagger=tagger)\n",
    "        for tag in blob.pos_tags:\n",
    "            key = tag[1]\n",
    "            if key in wtypebs:\n",
    "                wtypebs[key] += 1\n",
    "            else:\n",
    "                wtypebs[key] = 1\n",
    "        for key in wtypebs:\n",
    "            wbs.append(wtypebs[key])\n",
    "            wtbs.append(key)\n",
    "        sumwbs = sum(wbs) # Summe aller Wörter\n",
    "\n",
    "# drosten\n",
    "for key in wtypecd:\n",
    "    nv = wtypecd[key]/countercd\n",
    "    partswcd.update({key:nv})\n",
    "# ciesek\n",
    "for key in wtype:\n",
    "    nv = wtypesc[key]/countersc\n",
    "    partswsc.update({key:nv})\n",
    "# hennig\n",
    "for key in wtypekh:\n",
    "    nv = wtypekh[key]/counterkh\n",
    "    partswkh.update({key:nv})\n",
    "\n",
    "endresult = \n",
    "    + '\\n' \n",
    "    + '\\n' \n",
    "    + '\\n' \n",
    "    + 'Martini:\\nDurchschnittliche Redelänge: ' + str(avglenam) \n",
    "    + 'Verwendete Worttypen: ' + str(wtam) \n",
    "    + 'So häufig verwendet Martini...: ' + wtypeam + 'also anteilig...:' + str(partswam)\n",
    "    + '\\n' \n",
    "    + 'Schulmann:\\nDurchschnittliche Redelänge: ' + str(avglenbs) \n",
    "    + 'Verwendete Worttypen: ' + str(wtbs) \n",
    "    + 'So häufig verwendet Schulmann...: ' + wtypebs + 'also anteilig...:' + str(partswbs) \n",
    "\n",
    "\n",
    "# Ergebnisdoc erzeugen\n",
    "textdrosten = 'Drosten:\\nDurchschnittliche Redelänge: '+str(avglencd)+'Verwendete Worttypen: '+str(wtcd)+'So häufig verwendet Drosten...: '+str(wtypecd)+'also anteilig...:'+str(partswcd)\n",
    "textciesek = 'Ciesek:\\nDurchschnittliche Redelänge: '+str(avglensc)+'Verwendete Worttypen: '+str(wtsc)+'So häufig verwendet Ciesek...: '+str(wtypesc)+'also anteilig...:'+str(partswsc)\n",
    "texthennig = 'Hennig:\\nDurchschnittliche Redelänge: '+str(avglenkh)+'Verwendete Worttypen: '+str(wtkh)+'So häufig verwendet Hennig...: '+str(wtypekh)+'also anteilig...:'+str(partswkh)\n",
    "textmartini = 'Martini:\\nDurchschnittliche Redelänge: '+str(avglenam)+'Verwendete Worttypen: '+str(wtam)+'So häufig verwendet Martini...: '+str(wtypeam)+'also anteilig...:'+str(partswam)\n",
    "textschulmann = 'Schulmann:\\nDurchschnittliche Redelänge: '+str(avglenbs)+'Verwendete Worttypen: '+str(wtbs)+'So häufig verwendet Schulmann...: '+str(wtypebs)+'also anteilig...:'+str(partswbs)\n",
    "endresult = textdrosten + '\\n' + textciesek + '\\n' + texthennig + '\\n' + textmartini + '\\n' + textschulmann\n",
    "\n",
    "pathstats = os.path.join('data','STATS')\n",
    "with open(os.path.join(pathstats,\"results_textstats_ndr.txt\"),\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(endresult)\n",
    "\n",
    "# Piechart Redeanteil\n",
    "avglen = [avglencd,avglensc,avglenkh,avglenam,avglenbs]\n",
    "label = ['Christian Drosten','Sandra Ciesek','Korinna Hennig','Anja Martini','Beke Schulmann']\n",
    "print(avglen,label)\n",
    "\n",
    "plt.pie(avglen,labels=label)\n",
    "plt.legend(title=\"Redeanteil der Sprecher über alle Podcastfolgen\")\n",
    "plt.show()"
   ]
  }
 ]
}